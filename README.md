# MLWiki-Machine-Learning-and-Math-Notes

Some most common information during designing Machine Learning algorithms. Including must have Math background for each task. Feel free to add some updates here. Thanks. 


## Basics
-----------------------------
### Data Structures
<p align="center">
<img src="https://raw.githubusercontent.com/GensaGames/MLWiki-Machine-Learning-and-Math/master/images/Data_Sctructures.png" width="850" height="400" />
</p>
</br>

### Data Structure Algorithms
<p align="center">
<img src="https://raw.githubusercontent.com/GensaGames/MLWiki-Machine-Learning-and-Math/master/images/Data_Structures_Algorithms.png" width="850" height="500" />
</p>
</br>

-----------------------------
## Coursera Andrew Ng
All lecture slides, from source Cource of Andrew Ng. For more information, please check [Stanford Machine Learning Andrew Ng](http://www.holehouse.org/mlclass/), where described this Course in text format with detailed information.

#### Basics
* [Linear Algebra](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Linear%20Algebra.pdf)
[Linear Regression](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Linear%20Regression.pdf)</br>
[Linear Regression Multi](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Linear%20Regression%20Multi.pdf)
[Logistic Regression](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Logistic%20Regression.pdf)</br>
[Regularization](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Regularization.pdf)


#### Advanced 
* [Neural Network 1](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Neural-Network.pdf)
[Neural Network 2](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Neural-Network%202.pdf)</br>
[SVM](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/SVM.pdf)
[Clustering](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Clustring.pdf)


#### Other 
* [Advice Applying ML](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Advice%20for%20Applying%20ML.pdf)
[Dimensionality Reduction](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Dimensionality%20Reduction.pdf) </br>
[ML System Design](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/ML%20System%20Design.pdf)
[Recommender System](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Recommender%20System.pdf)


#### Application and Scaling 
* [Large Scale ML](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Large%20Scale%20ML.pdf)
[ML System Design](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/ML%20System%20Design.pdf)</br>
[ML System Example](https://github.com/GensaGames/MWiki-Machine-Learning-and-Math/blob/master/resources/ML-Andrew-Ng/Application%20Example.pdf)


-----------------------------
## Math  & Khan Academy

<p align="center">
<img src="https://raw.githubusercontent.com/GensaGames/MLWiki-Machine-Learning-and-Math/master/images/Math_for_Data_Science.png" width="850" height="550" />
</p>
</br>

#### Basics
* [Probability and combinatorics](https://www.khanacademy.org/math/precalculus/prob-comb)</br>
[Vectors](https://www.khanacademy.org/math/precalculus/vectors-precalc)
[Matrices](https://www.khanacademy.org/math/precalculus/precalc-matrices)


#### Differential Calculus & Multivariable calculus
* [Derivatives: definition and basic rules](https://www.khanacademy.org/math/differential-calculus/dc-diff-intro)
[Derivatives: chain rule and other advanced topics](https://www.khanacademy.org/math/differential-calculus/dc-chain)</br>
[Thinking about multivariable functions](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function)
[Derivatives of multivariable functions](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives)

#### Statistics & Probability
* [Analyzing categorical data](https://www.khanacademy.org/math/statistics-probability/analyzing-categorical-data)
[Displaying and comparing quantitative data](https://www.khanacademy.org/math/statistics-probability/displaying-describing-data)</br>
[Summarizing quantitative data](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data)




-----------------------------
## Other Resources

### 1. General


* [CS188 Artificial Intelligence (Reinforcement Learning and AI)](https://www.youtube.com/channel/UCshmLD2MsyqAKBx8ctivb5Q/videos) Great Youtube channel with introductino into Reinforcement Learning and AI in general. Instructor: Prof. Dan Klein.


### 2. Neural Network

* [RNN LSTM](https://www.youtube.com/watch?v=WCUNPb-5EYI) Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Very simple and friendly walk through how they work and how they are useful.

* [RNN](https://towardsdatascience.com/recurrent-neural-networks-and-lstm-4b601dd822a5) Introducation for the Recurrent Neural Network. This is because it is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for Machine Learning problems that involve sequential data. 



### 3. Optimization

* [SGD, Momentum, RMSProp](http://cs231n.github.io/neural-networks-3/) Nice general description for primary optimization objective in SGD, with Momentum, Nesterom, RMSProp and other. Also includes basic information during work with Neural Networks.

* [Momentum, RMSProp, Adagrad](https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/) Another good example describing optimization objective. Using simple Python example, authour showed optimization evolving, from one algorithm, to another. 


### 4. Activation

* [SoftMax, Sigmoid, ReLU](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions) Visualized sample of different activation functions and comparing Sigmoid and Softmax.

* [Simoid, ReLU, Tanh](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) The Nonlinear Activation Functions are the most used activation functions. It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.


### 5. Feature Extraction


* [SIFT (Scale-Invariant Feature Transform)](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html#sift-intro) So, in 2004, D.Lowe, University of British Columbia, came up with a new algorithm, Scale Invariant Feature Transform (SIFT) in his paper, Distinctive Image Features from Scale-Invariant Keypoints, which extract keypoints and compute its descriptors.

* [ORB (Oriented FAST and Rotated BRIEF)](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html#orb) This algorithm was brought up by Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary R. Bradski in their paper ORB: An efficient alternative to SIFT or SURF in 2011. As the title says, it is a good alternative to SIFT and SURF in computation cost, matching performance and mainly the patents. Yes, SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not!

* [SURF (Speeded-Up Robust Features)](https://www.youtube.com/watch?v=WCUNPb-5EYI) In 2006, three people, Bay, H., Tuytelaars, T. and Van Gool, L, published another paper, “SURF: Speeded Up Robust Features” which introduced a new algorithm called SURF. As name suggests, it is a speeded-up version of SIFT.

* [BRIEF (Binary Robust Independent Elementary Features)](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_brief/py_brief.html#brief) We know SIFT uses 128-dim vector for descriptors. Since it is using floating point numbers, it takes basically 512 bytes. Similarly SURF also takes minimum of 256 bytes (for 64-dim). Creating such a vector for thousands of features takes a lot of memory which are not feasible for resouce-constraint applications especially for embedded systems. Larger the memory, longer the time it takes for matching.



### 6. Reinforcement Learning


* [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts) We will first introduce several fundamental concepts and then dive into classic approaches to solving RL problems. Hopefully, this post could be a good starting point for newbies, bridging the future study on the cutting-edge research.


<p align="center">
<img src="https://raw.githubusercontent.com/GensaGames/MLWiki-Machine-Learning-and-Math/master/images/Reinforcement_Learning.png" width="850" height="400" />
</p>
</br>
